# ğŸ¤– LLM AI Chat App

Application de chat conversationnel desktop (Mac/Windows) utilisant votre IA LLM. OptimisÃ©e pour **Apple Silicon M3 Pro** avec support GPU Metal.

## âœ¨ CaractÃ©ristiques

- ğŸ’¬ **Chat conversationnel** en temps rÃ©el
- ğŸ **Apple Silicon M3 Pro** - utilise Metal GPU automatiquement
- ğŸ¨ **Interface moderne** en React/Tauri
- âš¡ **Ultra rapide** - app native compilÃ©e (pas d'Electron lourd)
- ğŸ”§ **ParamÃ¨tres ajustables** - modÃ¨le, tempÃ©rature, systÃ¨me prompt
- ğŸ’¾ **Historique** - conserve vos conversations
- ğŸŒ **Cross-platform** - Mac et Windows

## ğŸ“‹ PrÃ©requis

### Mac
- Node.js 18+ (https://nodejs.org)
- Python 3.10+ (via Homebrew: `brew install python`)
- Rust (pour Tauri): `curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh`
- Xcode Command Line Tools: `xcode-select --install`

### Windows
- Node.js 18+ (https://nodejs.org)
- Python 3.10+ (https://www.python.org)
- Rust: https://rustup.rs/
- Visual Studio Build Tools (ou Community Edition)

## ğŸš€ Installation rapide

### 1. Cloner ou crÃ©er le dossier
```bash
# Supposant que vous Ãªtes dans /Users/WilliamPro/Downloads/test
cd llm-app
```

### 2. Installer les dÃ©pendances frontend
```bash
npm install
```

### 3. Installer le backend Python
```bash
cd backend
pip install -r requirements.txt
cd ..
```

## â–¶ï¸ Lancer l'application

### Option 1 : DÃ©veloppement (avec rechargement auto)
```bash
# Terminal 1 - Lancer le backend LLM
cd backend
./run.sh  # macOS/Linux
# ou python main.py (Windows)

# Terminal 2 - Lancer l'app Tauri
npm run tauri:dev
```

### Option 2 : Production (app compilÃ©e)
```bash
# Construire l'app native
npm run tauri:build

# Mac: Trouvez l'app dans src-tauri/target/release/bundle/macos/
# Windows: Trouvez l'exe dans src-tauri/target/release/
```

## ğŸ Optimisation Apple Silicon M3 Pro

L'app utilise automatiquement **Metal GPU** si disponible :

```python
# Dans backend/main.py
if torch.backends.mps.is_available():
    device = "mps"  # Metal GPU
else:
    device = "cpu"
```

**Performance** :
- Avec GPU Metal: ~2-5 secondes par rÃ©ponse
- Sans GPU: ~5-10 secondes par rÃ©ponse (CPU)

## ğŸ“– Utilisation

1. **Lancer le serveur backend** (voir â–¶ï¸ Lancer)
2. **Ouvrir l'app** - interface Chat moderne
3. **Choisir le modÃ¨le** dans la barre latÃ©rale
4. **Ajuster les paramÃ¨tres**:
   - ğŸ¯ **TempÃ©rature** (0.1 = dÃ©terministe, 2.0 = alÃ©atoire)
   - ğŸ’¬ **Instructions systÃ¨me** (comportement de l'IA)
5. **Taper votre message** et appuyer sur `EntrÃ©e`

## ğŸ¯ ModÃ¨les disponibles

| ModÃ¨le | Taille | Vitesse | QualitÃ© |
|--------|--------|---------|---------|
| `sshleifer/tiny-gpt2` | ğŸŸ¢ TrÃ¨s lÃ©ger | âš¡âš¡âš¡ Rapide | ğŸŸ¡ Basique |
| `distilgpt2` | ğŸŸ¡ LÃ©ger | âš¡âš¡ Normal | ğŸŸ¢ Bon |
| `gpt2` | ğŸ”´ Standard | âš¡ Lent | ğŸŸ¢ Bon |

## ğŸ› ï¸ Architecture

```
llm-app/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ App.jsx          (Interface React)
â”‚   â”œâ”€â”€ App.css          (Style dark mode)
â”‚   â””â”€â”€ main.jsx         (Point d'entrÃ©e)
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py          (FastAPI serveur LLM)
â”‚   â”œâ”€â”€ requirements.txt  (DÃ©pendances Python)
â”‚   â””â”€â”€ run.sh           (Script lancement)
â”œâ”€â”€ package.json         (DÃ©pendances Node)
â”œâ”€â”€ vite.config.ts       (Config Vite)
â”œâ”€â”€ tauri.conf.json      (Config Tauri)
â””â”€â”€ index.html           (Template HTML)
```

## ğŸ› Troubleshooting

### âŒ "Erreur: Le serveur LLM n'est pas accessible"
**Solution** : Lancez d'abord le backend
```bash
cd backend
./run.sh  # macOS/Linux
python main.py  # Windows
```

### âŒ "ModuleNotFoundError: No module named 'fastapi'"
**Solution** : Installez les dÃ©pendances
```bash
cd backend
pip install -r requirements.txt
```

### âŒ "Erreur Metal GPU"
**Solution** : PyTorch Metal GPU peut Ãªtre instable, passez en CPU
```bash
# Dans le backend, dÃ©finir device = "cpu"
```

### âŒ Tauri build Ã©choue
**Solution** : Sur Mac, assurez-vous d'avoir Xcode tools:
```bash
xcode-select --install
```

## ğŸ“Š Performance M3 Pro

Tests avec `tiny-gpt2` sur MacBook Pro M3 Pro:
- **GÃ©nÃ©ration** : ~200-300 tokens/sec avec Metal
- **MÃ©moire** : ~500 MB
- **CPU** : 20-30% d'une core avec Metal

## ğŸ“ Fichiers importants

| Fichier | Description |
|---------|-------------|
| `backend/main.py` | Serveur FastAPI + LLM |
| `src/App.jsx` | Interface React |
| `tauri.conf.json` | Config app native |
| `package.json` | DÃ©pendances Node |
| `.venv/` | Environnement Python (crÃ©Ã© auto) |

## ğŸ” SÃ©curitÃ©

- Aucun modÃ¨le n'est envoyÃ© en ligne
- **Tout fonctionne localement** sur votre machine
- Aucun tracking ou donnÃ©es envoyÃ©es

## ğŸ“¦ Publier l'app

Pour partager l'app compilÃ©e :

### macOS
```bash
npm run tauri:build
# Compresse: src-tauri/target/release/bundle/macos/LLM\ AI\ Chat.app.tar.gz
```

### Windows
```bash
npm run tauri:build
# Fichier: src-tauri/target/release/LLM-AI-Chat_0.1.0_x64_en-US.msi
```

## ğŸ“„ Licence

MIT - libre d'utilisation

## ğŸ¤ Contribution

Les amÃ©liorations sont les bienvenues ! Fork, modifiez, push PR.

---

**CrÃ©Ã©e avec â¤ï¸ pour Apple Silicon M3 Pro**
