import argparse
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import time
import sys
from pathlib import Path
import json


# Cache pour les modÃ¨les chargÃ©s
_model_cache = {}


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Demo LLM conversationnel avancÃ© avec GPT-2",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""Exemples:
  python llm_demo.py --prompt "Bonjour"
  python llm_demo.py --mode chat  # mode interactif
  python llm_demo.py --model distilgpt2 --max-tokens 150 --system "Tu es un assistant"
  python llm_demo.py --save-history chat.json --enable-memory
        """,
    )
    parser.add_argument("--prompt", default="", help="Texte initial (ignore mode chat)")
    parser.add_argument(
        "--model",
        default="sshleifer/tiny-gpt2",
        help="ModÃ¨le Hugging Face (tiny-gpt2, distilgpt2, gpt2)",
    )
    parser.add_argument("--max-tokens", type=int, default=80, help="Tokens max gÃ©nÃ©rÃ©s")
    parser.add_argument(
        "--temperature", type=float, default=0.8, help="ContrÃ´le crÃ©ativitÃ© (0.1-2.0)"
    )
    parser.add_argument("--top-k", type=int, default=50, help="Top-k sampling")
    parser.add_argument("--top-p", type=float, default=0.95, help="Top-p (nucleus) sampling")
    parser.add_argument(
        "--mode",
        choices=["single", "chat"],
        default="single",
        help="single: un prompt, chat: conversation interactive",
    )
    parser.add_argument(
        "--system",
        default="",
        help="Instructions systÃ¨me pour faÃ§onner le comportement de l'IA"
    )
    parser.add_argument(
        "--save-history",
        default="",
        help="Sauvegarder l'historique du chat dans un fichier JSON"
    )
    parser.add_argument(
        "--load-history",
        default="",
        help="Charger l'historique d'un fichier JSON"
    )
    parser.add_argument(
        "--enable-memory",
        action="store_true",
        help="Activer la mÃ©moire long-terme (conserve les thÃ¨mes)"
    )
    return parser


def generate_text(generator, text: str, args) -> str:
    """GÃ©nÃ¨re du texte avec timing et contrÃ´le de qualitÃ©."""
    try:
        start = time.time()
        outputs = generator(
            text,
            max_new_tokens=args.max_tokens,
            do_sample=True,
            top_k=args.top_k,
            top_p=args.top_p,
            temperature=args.temperature,
            pad_token_id=50256,  # PrÃ©vient les avertissements
        )
        elapsed = time.time() - start
        result = outputs[0]["generated_text"]
        return result, elapsed
    except Exception as e:
        print(f"âŒ Erreur de gÃ©nÃ©ration: {e}", file=sys.stderr)
        return None, 0


def build_system_prompt(system_instruction: str, history_context: str = "") -> str:
    """Construit un prompt systÃ¨me amÃ©liorÃ©."""
    if system_instruction:
        base = system_instruction
    else:
        base = "Tu es un assistant IA utile, honnÃªte et inoffensif. Tu rÃ©pondras en franÃ§ais."
    
    if history_context:
        base += f"\n\nContexte prÃ©cÃ©dent:\n{history_context}"
    
    return base


def extract_keywords(text: str) -> list:
    """Extrait les mots clÃ©s importants pour la mÃ©moire long-terme."""
    common_words = {"le", "la", "de", "et", "ou", "est", "un", "une", "Ã ", "en", "je", "tu", "il", "elle"}
    words = text.lower().split()
    return [w for w in words if len(w) > 3 and w not in common_words]


def single_mode(args):
    """Mode avec un seul prompt."""
    if not args.prompt:
        args.prompt = "Bonjour, je suis une IA intelligente et je peux"

    print(f"\nğŸ“¦ Chargement du modÃ¨le '{args.model}'...")
    generator = pipeline("text-generation", model=args.model)

    print(f"âœ¨ GÃ©nÃ©ration en cours...\n")
    result, elapsed = generate_text(generator, args.prompt, args)

    if result:
        print(f"ğŸ“ Prompt:\n{args.prompt}")
        print(f"\nğŸ¤– RÃ©ponse:\n{result}")
        print(f"\nâ±ï¸  Temps: {elapsed:.2f}s | Tokens max: {args.max_tokens} | Temp: {args.temperature}")


def chat_mode(args):
    """Mode conversation multi-tour avec mÃ©moire et historique."""
    print(f"\nğŸ“¦ Chargement du modÃ¨le '{args.model}'...")
    generator = pipeline("text-generation", model=args.model)

    history = []
    memory = {"topics": [], "entities": []}
    token_count = 0
    start_time = time.time()
    
    # Charger historique si demandÃ©
    if args.load_history and Path(args.load_history).exists():
        try:
            with open(args.load_history) as f:
                data = json.load(f)
                history = data.get("history", [])
                memory = data.get("memory", memory)
            print(f"ğŸ“‚ Historique chargÃ© ({len(history)} messages)\n")
        except Exception as e:
            print(f"âš ï¸  Impossible de charger l'historique: {e}\n")

    print(
        "ğŸ’¬ Mode conversation (tapez 'quit' pour quitter, 'clear' pour rÃ©initialiser, 'mem' pour voir la mÃ©moire)\n"
    )

    while True:
        try:
            user_input = input("ğŸ‘¤ Vous: ").strip()
        except (EOFError, KeyboardInterrupt):
            break

        if user_input.lower() == "quit":
            break
        elif user_input.lower() == "clear":
            history = []
            memory = {"topics": [], "entities": []}
            token_count = 0
            print("ğŸ—‘ï¸  Historique et mÃ©moire effacÃ©s.\n")
            continue
        elif user_input.lower() == "mem":
            print(f"\nğŸ’¾ MÃ©moire: ThÃ¨mes={memory['topics'][-3:]}, EntitÃ©s={memory['entities'][-3:]}\n")
            continue
        elif not user_input:
            continue

        # Construit contexte avec historique (limitÃ©) et systÃ¨me
        context_msgs = " ".join([msg for pair in history[-2:] for msg in pair])
        system = build_system_prompt(args.system, context_msgs[:200] if context_msgs else "")
        prompt = f"{system}\n\nConversation:\n{context_msgs}\nğŸ‘¤ Vous: {user_input}\nğŸ¤– IA:"

        result, elapsed = generate_text(generator, prompt, args)

        if result:
            # Extrait rÃ©ponse nouvelle
            response = result.split("ğŸ¤– IA:")[-1].strip() if "ğŸ¤– IA:" in result else result
            response = response[:200]  # Limite la rÃ©ponse
            
            print(f"ğŸ¤– IA: {response}\n")
            history.append((user_input, response))
            
            # Mise Ã  jour de la mÃ©moire si activÃ©e
            if args.enable_memory:
                keywords = extract_keywords(user_input) + extract_keywords(response)
                memory["topics"].extend(keywords[-3:])
                memory["topics"] = list(set(memory["topics"]))[-10:]  # Garde 10 thÃ¨mes max
            
            token_count += args.max_tokens
        else:
            print("âš ï¸  Erreur lors de la gÃ©nÃ©ration.\n")

    # Sauvegarder historique si demandÃ©
    if args.save_history:
        try:
            with open(args.save_history, "w") as f:
                json.dump({"history": history, "memory": memory}, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ Historique sauvegardÃ© dans {args.save_history}")
        except Exception as e:
            print(f"\nâš ï¸  Impossible de sauvegarder: {e}")

    # Stats finales
    total_time = time.time() - start_time
    print(f"\nğŸ“Š Stats: {len(history)} Ã©changes, ~{token_count} tokens, {total_time:.1f}s total")
    print(f"ğŸ§  MÃ©moire: {len(memory['topics'])} thÃ¨mes conservÃ©s")


def main():
    args = build_parser().parse_args()

    if args.mode == "chat":
        chat_mode(args)
    else:
        single_mode(args)


if __name__ == "__main__":
    main()
