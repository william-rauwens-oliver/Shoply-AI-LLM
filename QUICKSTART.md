# ğŸš€ DÃ©marrage rapide - LLM AI Chat App

## 1ï¸âƒ£ Installation (une seule fois)

```bash
# Naviguer vers le dossier app
cd /Users/WilliamPro/Downloads/test/llm-app

# Installer les dÃ©pendances Node
npm install

# Installer les dÃ©pendances Python
cd backend
pip install -r requirements.txt
cd ..
```

**DurÃ©e** : ~5-10 minutes (selon la connexion internet)

## 2ï¸âƒ£ Lancer l'app

### âœ¨ La plus simple (macOS/Linux) :
```bash
cd /Users/WilliamPro/Downloads/test/llm-app
./start.sh
```

**C'est tout !** L'app s'ouvre automatiquement.

---

### ğŸ“Œ Alternative (2 terminaux) :

**Terminal 1 - Backend LLM** :
```bash
cd /Users/WilliamPro/Downloads/test/llm-app/backend
./run.sh
# Devrait afficher: âœ… Serveur lancÃ© sur http://localhost:7860
```

**Terminal 2 - Interface** :
```bash
cd /Users/WilliamPro/Downloads/test/llm-app
npm run tauri:dev
# L'app s'ouvre dans une fenÃªtre native
```

---

## 3ï¸âƒ£ Utilisation

1. **Interface apparaÃ®t** avec un chat noir/bleu moderne
2. **Choisissez le modÃ¨le** Ã  gauche (tiny-gpt2 par dÃ©faut = rapide)
3. **Tapez votre message** en bas
4. **Appuyez EntrÃ©e** ou cliquez ğŸ“¤
5. **L'IA rÃ©pond** en utilisant votre **M3 Pro Metal GPU** ğŸ

---

## âš¡ Performance sur M3 Pro

| ModÃ¨le | Temps | Utilisation GPU |
|--------|-------|-----------------|
| tiny-gpt2 | âš¡ 1-2 sec | ğŸŸ¢ TrÃ¨s lÃ©ger |
| distilgpt2 | âš¡âš¡ 2-4 sec | ğŸŸ¢ LÃ©ger |
| gpt2 | âš¡âš¡âš¡ 4-8 sec | ğŸŸ¡ Moyen |

---

## ğŸ› ï¸ DÃ©pannage

### âŒ "Le serveur LLM n'est pas accessible"
âœ… **Solution** : Lancez d'abord le backend dans Terminal 1

### âŒ App crashe au dÃ©marrage
âœ… **Solution** : 
```bash
# Supprimez et rÃ©installez les node_modules
rm -rf node_modules
npm install
```

### âŒ Python n'est pas trouvÃ©
âœ… **Solution** : Installez Python via Homebrew
```bash
brew install python
```

### âš ï¸ Premier chargement lent (normal)
- PremiÃ¨re fois : le modÃ¨le se tÃ©lÃ©charge (~200 MB)
- Prochaines fois : rapide (modÃ¨le en cache)

---

## ğŸ“‚ Structure

```
llm-app/
â”œâ”€â”€ backend/           â† Serveur LLM (FastAPI + PyTorch)
â”‚   â”œâ”€â”€ main.py       â† CÅ“ur du serveur
â”‚   â””â”€â”€ run.sh        â† Script lancement
â”œâ”€â”€ src/              â† Interface (React)
â”‚   â”œâ”€â”€ App.jsx       â† Chat UI
â”‚   â””â”€â”€ App.css       â† Styles
â”œâ”€â”€ start.sh          â† Lancement auto (tout en 1)
â”œâ”€â”€ README.md         â† Documentation complÃ¨te
â””â”€â”€ package.json      â† DÃ©pendances Node
```

---

## ğŸ Optimisation Apple Silicon

L'app dÃ©tecte automatiquement votre M3 Pro et utilise :
- âœ… **Metal GPU** si disponible (rapide)
- âœ… **CPU** en fallback (compatible)

Visible dans le backend log :
```
ğŸ Apple Silicon (Metal) dÃ©tectÃ© - utilisation du GPU
```

---

## ğŸ”§ Commandes utiles

```bash
# DÃ©veloppement avec rechargement auto
npm run tauri:dev

# Builder l'app native (Mac/Windows)
npm run tauri:build

# VÃ©rifier la santÃ© du serveur
curl http://localhost:7860/health

# Vider le cache des modÃ¨les
curl -X POST http://localhost:7860/api/clear-cache
```

---

## ğŸ“– Plus d'infos

- **README complet** : `llm-app/README.md`
- **Code frontend** : `llm-app/src/App.jsx`
- **Code backend** : `llm-app/backend/main.py`

---

## ğŸ’¡ Conseils

âœ¨ **Pour une meilleure expÃ©rience** :
1. Gardez le backend lancÃ© en arriÃ¨re-plan
2. Utilisez `tiny-gpt2` pour tester (trÃ¨s rapide)
3. Augmentez `TempÃ©rature` pour des rÃ©ponses plus crÃ©atives
4. Modifiez les "Instructions systÃ¨me" pour personnaliser l'IA

---

**Bon usage ! ğŸš€**
